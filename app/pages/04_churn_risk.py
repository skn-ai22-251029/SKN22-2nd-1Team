# import streamlit as st
# import pandas as pd
# import sys
# import os
# import plotly.graph_objects as go  # ì‹œê°í™”ë¥¼ ìœ„í•œ ì¶”ê°€
# from ui.header import render_header

# # =========================================================
# # [STEP 0] Streamlit ê¸°ë³¸ ì„¤ì •
# # - í˜ì´ì§€ ë‹¨ë… ì‹¤í–‰ ì‹œì—ë„ ê³µí†µ ë ˆì´ì•„ì›ƒ ìœ ì§€
# # =========================================================
# st.set_page_config(
#     page_title="Churn Risk Analysis",
#     page_icon="ğŸš¨",
#     layout="wide")

# # --- [STEP 1] ê²½ë¡œ ì„¤ì • ---
# current_dir = os.path.dirname(os.path.abspath(__file__))
# app_dir = os.path.abspath(os.path.join(current_dir, ".."))
# project_root = os.path.abspath(os.path.join(app_dir, ".."))

# if project_root not in sys.path:
#     sys.path.insert(0, project_root)
# if app_dir not in sys.path:
#     sys.path.insert(0, app_dir)

# # --- [STEP 2] ëª¨ë“ˆ ì„í¬íŠ¸ ---
# from service.CustomerCareCenter import PurchaseIntentService
# from adapters.model_loader import JoblibArtifactLoader
# from adapters.purchase_intent_pr_auc_adapter import PurchaseIntentPRAUCModelAdapter

# # --- [STEP 3] ë°ì´í„° ë° ì„œë¹„ìŠ¤ ë¡œë“œ ---
# @st.cache_resource
# def init_service():
#     model_path = "artifacts/best_pr_auc_balancedrf.joblib"
#     adapter = PurchaseIntentPRAUCModelAdapter(model_path)
#     return PurchaseIntentService(adapter), adapter

# @st.cache_data
# def load_data():
#     return pd.read_csv("data/processed/test.csv")

# service, adapter = init_service()
# df = load_data()

# # =========================================================
# # [ì¶”ê°€] ê°œë°œ ì¤‘ ì½”ë“œ/ë©”ì‹œì§€ ë³€ê²½ì´ ë°˜ì˜ ì•ˆ ë  ë•Œë¥¼ ëŒ€ë¹„í•œ ìºì‹œ ì´ˆê¸°í™” ë²„íŠ¼
# # - st.cache_resource ë•Œë¬¸ì— "ì„œë¹„ìŠ¤ ê°ì²´"ê°€ ì˜¤ë˜ ì‚´ì•„ë‚¨ì•„
# #   ìˆ˜ì •í•œ recommend_actionì´ ì¦‰ì‹œ ë°˜ì˜ë˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆìŒ
# # =========================================================
# with st.sidebar:
#     if st.button("ìºì‹œ ì´ˆê¸°í™”(ê°œë°œìš©)"):
#         st.cache_data.clear()
#         st.cache_resource.clear()
#         st.rerun()
# # =========================================================

# # =========================================================
# # [ê¸°ì¡´ ìœ ì§€] ìƒíƒœëª… ë§¤í•‘
# # =========================================================
# RISK_NAME_MAP = {
#     "HIGH_RISK": "ê³ ìœ„í—˜ ì´íƒˆêµ°",
#     "OPPORTUNITY": "ì „í™˜ ê¸°íšŒêµ°",
#     "LIKELY_BUYER": "êµ¬ë§¤ ìœ ë ¥êµ°",
# }

# # =========================================================
# # [ê¸°ì¡´ ìœ ì§€] ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ êµ¬ë§¤í™•ë¥ /ìœ„í—˜ë“±ê¸‰ ê³„ì‚°
# # =========================================================
# @st.cache_data
# def compute_scores(df_all: pd.DataFrame) -> pd.DataFrame:
#     X_all = df_all.drop(columns=["Revenue"], errors="ignore")
#     proba_series = adapter.predict_proba(X_all)

#     if hasattr(proba_series, "values"):
#         proba_values = proba_series.values
#         idx_values = df_all.index
#         proba_s = pd.Series(proba_values, index=idx_values, name="purchase_proba")
#     else:
#         proba_s = pd.Series(proba_series, index=df_all.index, name="purchase_proba")

#     risk_codes = proba_s.apply(lambda p: service.classify_risk(float(p)))

#     score_df = pd.DataFrame({
#         "purchase_proba": proba_s,
#         "risk_code": risk_codes
#     }, index=df_all.index)

#     return score_df

# # =========================================================
# # [ê¸°ì¡´ ìœ ì§€] "ê³ ìœ„í—˜ 5 / ê¸°íšŒ 3 / ìœ ë ¥ 2"ë¡œ 10ê°œ ì„¸ì…˜ ì„ ì •
# # =========================================================
# def select_10_sessions(score_df: pd.DataFrame) -> list[int]:
#     high_needed, opp_needed, likely_needed = 5, 3, 2

#     high_df = score_df[score_df["risk_code"] == "HIGH_RISK"].sort_values("purchase_proba", ascending=True)
#     opp_df = score_df[score_df["risk_code"] == "OPPORTUNITY"].sort_values("purchase_proba", ascending=False)
#     likely_df = score_df[score_df["risk_code"] == "LIKELY_BUYER"].sort_values("purchase_proba", ascending=False)

#     selected_idx = []
#     selected_idx += list(high_df.head(high_needed).index)
#     selected_idx += list(opp_df.head(opp_needed).index)
#     selected_idx += list(likely_df.head(likely_needed).index)

#     if len(selected_idx) < 10:
#         remaining = score_df.drop(index=selected_idx, errors="ignore").sort_values("purchase_proba", ascending=False)
#         need = 10 - len(selected_idx)
#         selected_idx += list(remaining.head(need).index)

#     return selected_idx[:10]

# # =========================================================
# # [ê¸°ì¡´ ìœ ì§€] ë“œë¡­ë‹¤ìš´ ë¼ë²¨ ìƒì„±
# # [ìˆ˜ì •] label -> group_id(1~10) ë§¤í•‘ì„ ì¶”ê°€ë¡œ ë§Œë“ ë‹¤
# # =========================================================
# score_df = compute_scores(df)
# selected_idx_list = select_10_sessions(score_df)

# df_selected = df.loc[selected_idx_list].copy()
# score_selected = score_df.loc[selected_idx_list]

# group_label_map = {}  # label -> real_idx
# group_id_map = {}     # [ìˆ˜ì •/ì¶”ê°€] label -> group_id(1~10)

# for i, real_idx in enumerate(df_selected.index, start=1):
#     risk_code = score_selected.loc[real_idx, "risk_code"]
#     risk_name = RISK_NAME_MAP.get(risk_code, "ê´€ì°° í•„ìš”")
#     label = f"ê·¸ë£¹{i}({risk_name})"

#     group_label_map[label] = real_idx
#     group_id_map[label] = i  # [ìˆ˜ì •/ì¶”ê°€] í•µì‹¬: UI ê·¸ë£¹ë²ˆí˜¸ë¥¼ ì €ì¥

# # --- [STEP 4] UI ë ˆì´ì•„ì›ƒ ì„¤ì • ---
# render_header()
# st.title("ğŸ›¡ï¸ ê³ ê° ì´íƒˆ ë°©ì§€ ë° ë§ˆì¼€íŒ… ì „ëµ ê°€ì´ë“œ")

# left_col, right_col = st.columns([4, 6])

# with left_col:
#     st.subheader("ğŸ“ ì„¸ì…˜ ì •ë³´ ì…ë ¥")

#     selected_label = st.selectbox(
#         "ë¶„ì„í•  ê³ ê° ì„¸ì…˜ ì„ íƒ",
#         options=list(group_label_map.keys()),
#         key="session_select_group"
#     )

#     idx = group_label_map[selected_label]
#     selected_group_id = group_id_map[selected_label]  # [ìˆ˜ì •/ì¶”ê°€] ì„ íƒëœ ê·¸ë£¹ë²ˆí˜¸(1~10)

#     row = df.loc[idx]

#     st.write("---")
#     st.write("**ğŸ“ ì£¼ìš” í–‰ë™ ì§€í‘œ**")
#     st.write(f"- í˜ì´ì§€ ê°€ì¹˜: `{row.get('PageValues', 0):.2f}`")
#     st.write(f"- ì´íƒˆë¥ : `{row.get('BounceRates', 0)*100:.1f}%`")
#     st.write(f"- ì²´ë¥˜ ì‹œê°„: `{row.get('ProductRelated_Duration', 0):.0f}ì´ˆ`")

#     X_one = pd.DataFrame([row.drop("Revenue", errors="ignore")])
#     proba = float(adapter.predict_proba(X_one).iloc[0])
#     risk = service.classify_risk(proba)

#     # =========================================================
#     # [ìˆ˜ì •] group_id(1~10)ë¥¼ recommend_actionì— ì „ë‹¬
#     # - ì´ì œ ê·¸ë£¹ ì„ íƒì— ë”°ë¼ 10ì¢… ë©”ì‹œì§€ê°€ 1:1ë¡œ ë°”ë€œ
#     # =========================================================
#     action = service.recommend_action(row.to_dict(), proba, group_id=selected_group_id)

# with right_col:
#     st.subheader("ğŸ“Š ë¶„ì„ ê²°ê³¼ ë° ì‹œê°í™”")

#     fig = go.Figure(go.Indicator(
#         mode="gauge+number",
#         value=proba * 100,
#         domain={'x': [0, 1], 'y': [0, 1]},
#         title={'text': "êµ¬ë§¤ ì „í™˜ í™•ë¥  (%)", 'font': {'size': 20}},
#         gauge={
#             'axis': {'range': [None, 100], 'tickwidth': 1},
#             'bar': {'color': "#1f77b4"},
#             'steps': [
#                 {'range': [0, 20], 'color': "#ff4b4b"},
#                 {'range': [20, 60], 'color': "#ffa500"},
#                 {'range': [60, 100], 'color': "#28a745"}
#             ],
#             'threshold': {
#                 'line': {'color': "white", 'width': 4},
#                 'thickness': 0.75,
#                 'value': proba * 100
#             }
#         }
#     ))
#     fig.update_layout(height=350, margin=dict(l=20, r=20, t=50, b=20))
#     st.plotly_chart(fig, use_container_width=True)

#     if risk == "HIGH_RISK":
#         st.error(f"ğŸš¨ **ìƒíƒœ: ê³ ìœ„í—˜ ì´íƒˆêµ°** (í™•ë¥ : {proba*100:.1f}%)")
#     elif risk == "OPPORTUNITY":
#         st.warning(f"âš ï¸ **ìƒíƒœ: ì „í™˜ ê¸°íšŒêµ°** (í™•ë¥ : {proba*100:.1f}%)")
#     else:
#         st.success(f"âœ… **ìƒíƒœ: êµ¬ë§¤ ìœ ë ¥êµ°** (í™•ë¥ : {proba*100:.1f}%)")

#     st.info(f"ğŸ’¡ **ì¶”ì²œ ë§ˆì¼€íŒ… ì•¡ì…˜:**\n\n{action}")

# with st.expander("â„¹ï¸ ë¶„ì„ ê¸°ì¤€ ë° íƒ€ê²ŸíŒ… ë¡œì§ ì•ˆë‚´"):
#     st.markdown("""
#     **ë¶„ì„ ëŒ€ìƒ ì„ ì • ê¸°ì¤€(ì´ 10ê°œ ì„¸ì…˜):**
#     * **ì „ì²´ ì˜ˆì¸¡ ê¸°ë°˜ ìƒ˜í”Œë§**: í…ŒìŠ¤íŠ¸ ë°ì´í„°(`test.csv`) ì „ì²´ ì„¸ì…˜ì— ëŒ€í•´ ëª¨ë¸ì´ **êµ¬ë§¤ ì „í™˜ í™•ë¥ (purchase_proba)** ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
#     * **ìœ„í—˜ ë“±ê¸‰ ë¶„ë¥˜**:
#       - **ê³ ìœ„í—˜ ì´íƒˆêµ°(HIGH_RISK)**: `p < 0.20`
#       - **ì „í™˜ ê¸°íšŒêµ°(OPPORTUNITY)**: `0.20 â‰¤ p < 0.60`
#       - **êµ¬ë§¤ ìœ ë ¥êµ°(LIKELY_BUYER)**: `p â‰¥ 0.60`
#     * **ê·¸ë£¹ êµ¬ì„± ë¹„ìœ¨(ë°ëª¨ìš©)**: **ê³ ìœ„í—˜ 5 / ê¸°íšŒ 3 / ìœ ë ¥ 2**ë¡œ ì´ 10ê°œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
#     * **ëŒ€í‘œ ì„¸ì…˜ ì„ ì • ë°©ì‹**
#       - ê³ ìœ„í—˜ 5ê°œ: HIGH_RISK ì¤‘ **êµ¬ë§¤í™•ë¥ ì´ ê°€ì¥ ë‚®ì€ 5ê°œ**
#       - ê¸°íšŒ 3ê°œ: OPPORTUNITY ì¤‘ **êµ¬ë§¤í™•ë¥ ì´ ê°€ì¥ ë†’ì€ 3ê°œ**
#       - ìœ ë ¥ 2ê°œ: LIKELY_BUYER ì¤‘ **êµ¬ë§¤í™•ë¥ ì´ ê°€ì¥ ë†’ì€ 2ê°œ**
#     * **ì¤‘ìš”**: ë“œë¡­ë‹¤ìš´ì˜ â€œê·¸ë£¹1~10â€ì€ ìœ„ ê·œì¹™ìœ¼ë¡œ ë½‘íŒ **í‘œë³¸ì˜ ìˆœë²ˆ**ì´ë©°,
#       ì´ ìˆœë²ˆ(1~10)ì„ ì„œë¹„ìŠ¤ë¡œ ì „ë‹¬í•˜ì—¬ **10ì¢… ë©”ì‹œì§€ë¥¼ 1:1ë¡œ ì¶œë ¥**í•©ë‹ˆë‹¤.
#     """)

# with st.expander("ğŸ” ìƒì„¸ ì„¸ì…˜ ë°ì´í„° ë³´ê¸°"):
#     st.dataframe(pd.DataFrame([row]))

# st.info("ğŸ’¡ ë°ëª¨ íš¨ìœ¨ì„ ìœ„í•´, ëª¨ë¸ ì˜ˆì¸¡ ê¸°ë°˜ìœ¼ë¡œ 10ê°œ ì„¸ì…˜ì„ (ê³ ìœ„í—˜ 5 / ê¸°íšŒ 3 / ìœ ë ¥ 2)ë¡œ êµ¬ì„±í•´ ì œê³µí•©ë‹ˆë‹¤.")



import streamlit as st
import pandas as pd
import sys
import os
import plotly.graph_objects as go  # ì‹œê°í™”ë¥¼ ìœ„í•œ ì¶”ê°€
from ui.header import render_header

# =========================================================
# [STEP 0] Streamlit ê¸°ë³¸ ì„¤ì •
# - í˜ì´ì§€ ë‹¨ë… ì‹¤í–‰ ì‹œì—ë„ ê³µí†µ ë ˆì´ì•„ì›ƒ ìœ ì§€
# =========================================================
st.set_page_config(
    page_title="Churn Risk Analysis",
    page_icon="ğŸš¨",
    layout="wide"
)

# =========================================================
# [STEP 1] ê²½ë¡œ ì„¤ì •
# =========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
app_dir = os.path.abspath(os.path.join(current_dir, ".."))
project_root = os.path.abspath(os.path.join(app_dir, ".."))

if project_root not in sys.path:
    sys.path.insert(0, project_root)
if app_dir not in sys.path:
    sys.path.insert(0, app_dir)

# =========================================================
# [STEP 2] ëª¨ë“ˆ ì„í¬íŠ¸
# =========================================================
from service.CustomerCareCenter import PurchaseIntentService
from adapters.model_loader import JoblibArtifactLoader
from adapters.purchase_intent_pr_auc_adapter import PurchaseIntentPRAUCModelAdapter

# =========================================================
# [STEP 3] ë°ì´í„° ë° ì„œë¹„ìŠ¤ ë¡œë“œ
# - âœ… PurchaseIntentServiceê°€ artifact_pathë¥¼ ìš”êµ¬í•˜ë¯€ë¡œ ë°˜ë“œì‹œ ì „ë‹¬
# - âœ… artifactë„ í•¨ê»˜ ë¡œë“œí•´ì„œ ëª¨ë¸ ì…ë ¥ ìŠ¤í‚¤ë§ˆ(feature_names_in_) ì¶”ì¶œ
# =========================================================
@st.cache_resource
def init_service():
    model_path = os.path.join(app_dir, "artifacts", "best_pr_auc_balancedrf.joblib")

    if not os.path.exists(model_path):
        st.error(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        st.stop()

    # AdapterëŠ” ê²½ë¡œ ê¸°ë°˜ìœ¼ë¡œ ë¡œë“œ (íŒ€ ì–´ëŒ‘í„° êµ¬í˜„ ê¸°ì¤€)
    adapter = PurchaseIntentPRAUCModelAdapter(model_path)

    # ServiceëŠ” artifact_path í•„ìš” (ì§€ê¸ˆ ë„ˆí¬ CustomerCareCenter ìµœì¢… êµ¬ì¡° ê¸°ì¤€)
    service = PurchaseIntentService(adapter=adapter, artifact_path=model_path)

    # artifactë„ ë¡œë“œí•´ì„œ pipeline/meta í™•ì¸ ê°€ëŠ¥í•˜ê²Œ ë³´ê´€
    loader = JoblibArtifactLoader(model_path)
    artifact = loader.load()

    return service, adapter, artifact


@st.cache_data
def load_data():
    data_path = os.path.join(project_root, "data", "processed", "test.csv")
    if not os.path.exists(data_path):
        st.error(f"âŒ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
        st.stop()
    return pd.read_csv(data_path)


service, adapter, artifact = init_service()
df = load_data()

# =========================================================
# [ì¶”ê°€] ëª¨ë¸ ê¸°ì¤€ feature ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ + ì…ë ¥ ì •ë ¬
# - âœ… "ì…ë ¥ DataFrameì„ ëª¨ë¸ê¸°ì¤€ìœ¼ë¡œ ë§ì¶°ì¤˜" ìš”êµ¬ì‚¬í•­ ë°˜ì˜
# - UI/ë¬¸êµ¬/ê·¸ë˜í”„ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  ë‚´ë¶€ ì…ë ¥ë§Œ ì •ë ¬
# =========================================================
@st.cache_data
def get_expected_columns(df_sample: pd.DataFrame) -> list[str]:
    # 1) adapter.pipeline.feature_names_in_ ìš°ì„ 
    if hasattr(adapter, "pipeline") and hasattr(adapter.pipeline, "feature_names_in_"):
        return list(adapter.pipeline.feature_names_in_)

    # 2) artifact.pipeline.feature_names_in_
    if hasattr(artifact, "pipeline") and hasattr(artifact.pipeline, "feature_names_in_"):
        return list(artifact.pipeline.feature_names_in_)

    # 3) artifact.metaì— feature ë¦¬ìŠ¤íŠ¸ê°€ ì €ì¥ëœ ê²½ìš°
    if hasattr(artifact, "meta") and isinstance(artifact.meta, dict):
        for k in ["feature_cols", "feature_columns", "columns", "X_columns"]:
            if k in artifact.meta:
                return list(artifact.meta[k])

    # 4) ìµœí›„ fallback: dfì—ì„œ Revenue ì œì™¸
    return [c for c in df_sample.columns if c != "Revenue"]


EXPECTED_COLS = get_expected_columns(df)


def align_to_model_schema(row_or_df):
    """
    row(Series) ë˜ëŠ” df(DataFrame)ë¥¼ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” feature ì»¬ëŸ¼(EXPECTED_COLS)ì— ë§ì¶˜ë‹¤.
    - ì—†ëŠ” ì»¬ëŸ¼ì€ 0ìœ¼ë¡œ ì±„ì›€
    - ì¶”ê°€ ì»¬ëŸ¼ì€ ì œê±°
    - ì»¬ëŸ¼ ìˆœì„œë„ ëª¨ë¸ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    """
    if isinstance(row_or_df, pd.Series):
        X = pd.DataFrame([row_or_df.drop("Revenue", errors="ignore")])
    else:
        X = row_or_df.drop(columns=["Revenue"], errors="ignore").copy()

    for col in EXPECTED_COLS:
        if col not in X.columns:
            X[col] = 0

    return X[EXPECTED_COLS]

# =========================================================
# [ì¶”ê°€] ê°œë°œ ì¤‘ ì½”ë“œ/ë©”ì‹œì§€ ë³€ê²½ì´ ë°˜ì˜ ì•ˆ ë  ë•Œë¥¼ ëŒ€ë¹„í•œ ìºì‹œ ì´ˆê¸°í™” ë²„íŠ¼
# =========================================================
with st.sidebar:
    if st.button("ìºì‹œ ì´ˆê¸°í™”(ê°œë°œìš©)"):
        st.cache_data.clear()
        st.cache_resource.clear()
        st.rerun()

# =========================================================
# [ê¸°ì¡´ ìœ ì§€] ìƒíƒœëª… ë§¤í•‘
# =========================================================
RISK_NAME_MAP = {
    "HIGH_RISK": "ê³ ìœ„í—˜ ì´íƒˆêµ°",
    "OPPORTUNITY": "ì „í™˜ ê¸°íšŒêµ°",
    "LIKELY_BUYER": "êµ¬ë§¤ ìœ ë ¥êµ°",
}

# =========================================================
# [ê¸°ì¡´ ìœ ì§€] ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ êµ¬ë§¤í™•ë¥ /ìœ„í—˜ë“±ê¸‰ ê³„ì‚°
# - âœ… ì…ë ¥ì€ align_to_model_schemaë¡œ ëª¨ë¸ ê¸°ì¤€ ì •ë ¬
# =========================================================
@st.cache_data
def compute_scores(df_all: pd.DataFrame) -> pd.DataFrame:
    X_all = align_to_model_schema(df_all)
    proba_series = adapter.predict_proba(X_all)

    if hasattr(proba_series, "values"):
        proba_values = proba_series.values
        idx_values = df_all.index
        proba_s = pd.Series(proba_values, index=idx_values, name="purchase_proba")
    else:
        proba_s = pd.Series(proba_series, index=df_all.index, name="purchase_proba")

    # âœ… service.classify_riskê°€ CustomerCareCenterì— ì¶”ê°€ëœ ìƒíƒœì—¬ì•¼ í•¨
    risk_codes = proba_s.apply(lambda p: service.classify_risk(float(p)))

    score_df = pd.DataFrame({
        "purchase_proba": proba_s,
        "risk_code": risk_codes
    }, index=df_all.index)

    return score_df

# =========================================================
# [ê¸°ì¡´ ìœ ì§€] "ê³ ìœ„í—˜ 5 / ê¸°íšŒ 3 / ìœ ë ¥ 2"ë¡œ 10ê°œ ì„¸ì…˜ ì„ ì •
# =========================================================
def select_10_sessions(score_df: pd.DataFrame) -> list[int]:
    high_needed, opp_needed, likely_needed = 5, 3, 2

    high_df = score_df[score_df["risk_code"] == "HIGH_RISK"].sort_values("purchase_proba", ascending=True)
    opp_df = score_df[score_df["risk_code"] == "OPPORTUNITY"].sort_values("purchase_proba", ascending=False)
    likely_df = score_df[score_df["risk_code"] == "LIKELY_BUYER"].sort_values("purchase_proba", ascending=False)

    selected_idx = []
    selected_idx += list(high_df.head(high_needed).index)
    selected_idx += list(opp_df.head(opp_needed).index)
    selected_idx += list(likely_df.head(likely_needed).index)

    if len(selected_idx) < 10:
        remaining = score_df.drop(index=selected_idx, errors="ignore").sort_values("purchase_proba", ascending=False)
        need = 10 - len(selected_idx)
        selected_idx += list(remaining.head(need).index)

    return selected_idx[:10]

# =========================================================
# [ê¸°ì¡´ ìœ ì§€] ë“œë¡­ë‹¤ìš´ ë¼ë²¨ ìƒì„±
# [ìœ ì§€] label -> group_id(1~10) ë§¤í•‘
# =========================================================
score_df = compute_scores(df)
selected_idx_list = select_10_sessions(score_df)

df_selected = df.loc[selected_idx_list].copy()
score_selected = score_df.loc[selected_idx_list]

group_label_map = {}  # label -> real_idx
group_id_map = {}     # label -> group_id(1~10)

for i, real_idx in enumerate(df_selected.index, start=1):
    risk_code = score_selected.loc[real_idx, "risk_code"]
    risk_name = RISK_NAME_MAP.get(risk_code, "ê´€ì°° í•„ìš”")
    label = f"ê·¸ë£¹{i}({risk_name})"

    group_label_map[label] = real_idx
    group_id_map[label] = i  # í•µì‹¬: UI ê·¸ë£¹ë²ˆí˜¸ë¥¼ ì €ì¥

# =========================================================
# [STEP 4] UI ë ˆì´ì•„ì›ƒ ì„¤ì •
# =========================================================
render_header()
st.title("ğŸ›¡ï¸ ê³ ê° ì´íƒˆ ë°©ì§€ ë° ë§ˆì¼€íŒ… ì „ëµ ê°€ì´ë“œ")

left_col, right_col = st.columns([4, 6])

with left_col:
    st.subheader("ğŸ“ ì„¸ì…˜ ì •ë³´ ì…ë ¥")

    selected_label = st.selectbox(
        "ë¶„ì„í•  ê³ ê° ì„¸ì…˜ ì„ íƒ",
        options=list(group_label_map.keys()),
        key="session_select_group"
    )

    idx = group_label_map[selected_label]
    selected_group_id = group_id_map[selected_label]  # ì„ íƒëœ ê·¸ë£¹ë²ˆí˜¸(1~10)

    row = df.loc[idx]

    st.write("---")
    st.write("**ğŸ“ ì£¼ìš” í–‰ë™ ì§€í‘œ**")
    st.write(f"- í˜ì´ì§€ ê°€ì¹˜: `{row.get('PageValues', 0):.2f}`")
    st.write(f"- ì´íƒˆë¥ : `{row.get('BounceRates', 0)*100:.1f}%`")
    st.write(f"- ì²´ë¥˜ ì‹œê°„: `{row.get('ProductRelated_Duration', 0):.0f}ì´ˆ`")

    # âœ… ë‹¨ì¼ rowë„ ëª¨ë¸ ê¸°ì¤€ ì»¬ëŸ¼ ì •ë ¬
    X_one = align_to_model_schema(row)
    proba = float(adapter.predict_proba(X_one).iloc[0])
    risk = service.classify_risk(proba)

    # group_id(1~10)ë¥¼ recommend_actionì— ì „ë‹¬
    action = service.recommend_action(row.to_dict(), proba, group_id=selected_group_id)

with right_col:
    st.subheader("ğŸ“Š ë¶„ì„ ê²°ê³¼ ë° ì‹œê°í™”")

    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=proba * 100,
        domain={'x': [0, 1], 'y': [0, 1]},
        title={'text': "êµ¬ë§¤ ì „í™˜ í™•ë¥  (%)", 'font': {'size': 20}},
        gauge={
            'axis': {'range': [None, 100], 'tickwidth': 1},
            'bar': {'color': "#1f77b4"},
            'steps': [
                {'range': [0, 20], 'color': "#ff4b4b"},
                {'range': [20, 60], 'color': "#ffa500"},
                {'range': [60, 100], 'color': "#28a745"}
            ],
            'threshold': {
                'line': {'color': "white", 'width': 4},
                'thickness': 0.75,
                'value': proba * 100
            }
        }
    ))
    fig.update_layout(height=350, margin=dict(l=20, r=20, t=50, b=20))
    st.plotly_chart(fig, use_container_width=True)

    if risk == "HIGH_RISK":
        st.error(f"ğŸš¨ **ìƒíƒœ: ê³ ìœ„í—˜ ì´íƒˆêµ°** (í™•ë¥ : {proba*100:.1f}%)")
    elif risk == "OPPORTUNITY":
        st.warning(f"âš ï¸ **ìƒíƒœ: ì „í™˜ ê¸°íšŒêµ°** (í™•ë¥ : {proba*100:.1f}%)")
    else:
        st.success(f"âœ… **ìƒíƒœ: êµ¬ë§¤ ìœ ë ¥êµ°** (í™•ë¥ : {proba*100:.1f}%)")

    st.info(f"ğŸ’¡ **ì¶”ì²œ ë§ˆì¼€íŒ… ì•¡ì…˜:**\n\n{action}")

# =========================================================
# [ìœ ì§€] ë¶„ì„ ê¸°ì¤€ ë° íƒ€ê²ŸíŒ… ë¡œì§ ì•ˆë‚´ (Expander)
# =========================================================
with st.expander("â„¹ï¸ ë¶„ì„ ê¸°ì¤€ ë° íƒ€ê²ŸíŒ… ë¡œì§ ì•ˆë‚´"):
    st.markdown("""
    **ë¶„ì„ ëŒ€ìƒ ì„ ì • ê¸°ì¤€(ì´ 10ê°œ ì„¸ì…˜):**
    * **ì „ì²´ ì˜ˆì¸¡ ê¸°ë°˜ ìƒ˜í”Œë§**: í…ŒìŠ¤íŠ¸ ë°ì´í„°(`test.csv`) ì „ì²´ ì„¸ì…˜ì— ëŒ€í•´ ëª¨ë¸ì´ **êµ¬ë§¤ ì „í™˜ í™•ë¥ (purchase_proba)** ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    * **ìœ„í—˜ ë“±ê¸‰ ë¶„ë¥˜**:
      - **ê³ ìœ„í—˜ ì´íƒˆêµ°(HIGH_RISK)**: `p < 0.20`
      - **ì „í™˜ ê¸°íšŒêµ°(OPPORTUNITY)**: `0.20 â‰¤ p < 0.60`
      - **êµ¬ë§¤ ìœ ë ¥êµ°(LIKELY_BUYER)**: `p â‰¥ 0.60`
    * **ê·¸ë£¹ êµ¬ì„± ë¹„ìœ¨(ë°ëª¨ìš©)**: **ê³ ìœ„í—˜ 5 / ê¸°íšŒ 3 / ìœ ë ¥ 2**ë¡œ ì´ 10ê°œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    * **ëŒ€í‘œ ì„¸ì…˜ ì„ ì • ë°©ì‹**
      - ê³ ìœ„í—˜ 5ê°œ: HIGH_RISK ì¤‘ **êµ¬ë§¤í™•ë¥ ì´ ê°€ì¥ ë‚®ì€ 5ê°œ**
      - ê¸°íšŒ 3ê°œ: OPPORTUNITY ì¤‘ **êµ¬ë§¤í™•ë¥ ì´ ê°€ì¥ ë†’ì€ 3ê°œ**
      - ìœ ë ¥ 2ê°œ: LIKELY_BUYER ì¤‘ **êµ¬ë§¤í™•ë¥ ì´ ê°€ì¥ ë†’ì€ 2ê°œ**
    * **ì¤‘ìš”**: ë“œë¡­ë‹¤ìš´ì˜ â€œê·¸ë£¹1~10â€ì€ ìœ„ ê·œì¹™ìœ¼ë¡œ ë½‘íŒ **í‘œë³¸ì˜ ìˆœë²ˆ**ì´ë©°,
      ì´ ìˆœë²ˆ(1~10)ì„ ì„œë¹„ìŠ¤ë¡œ ì „ë‹¬í•˜ì—¬ **10ì¢… ë©”ì‹œì§€ë¥¼ 1:1ë¡œ ì¶œë ¥**í•©ë‹ˆë‹¤.
    """)

# =========================================================
# [ìœ ì§€] ìƒì„¸ ì„¸ì…˜ ë°ì´í„° ë³´ê¸° (Expander)
# =========================================================
with st.expander("ğŸ” ìƒì„¸ ì„¸ì…˜ ë°ì´í„° ë³´ê¸°"):
    st.dataframe(pd.DataFrame([row]))

# =========================================================
# [ìœ ì§€] ë§ˆì§€ë§‰ ì•ˆë‚´ ë¬¸êµ¬
# =========================================================
st.info("ğŸ’¡ ë°ëª¨ íš¨ìœ¨ì„ ìœ„í•´, ëª¨ë¸ ì˜ˆì¸¡ ê¸°ë°˜ìœ¼ë¡œ 10ê°œ ì„¸ì…˜ì„ (ê³ ìœ„í—˜ 5 / ê¸°íšŒ 3 / ìœ ë ¥ 2)ë¡œ êµ¬ì„±í•´ ì œê³µí•©ë‹ˆë‹¤.")
